# -*- coding: utf-8 -*-
"""Turo modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IGdU4SUk4d9xwR_qc24tFmWQ0U9B_HaK

# Datasource Preparation

## Extrat Turo date from json file and normalize the dictonaries to dataframe
"""

import json 
import pandas as pd 
from pandas.io.json import json_normalize 
df = pd.read_json('/content/drive/Shareddrives/ML&DD/final project/database.json')
df.head()
owner_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in df['owner']],ignore_index=True)
owner_df.head()
rate_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in df['rate']],ignore_index=True)
rate_df.head()
### distanceWithUnit
distanceWithUnit_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in df['distanceWithUnit']],ignore_index=True)
distanceWithUnit_df.head()
### location
location_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in df['location']],ignore_index=True)
location_df.head()
### vehicle
vehicle_df = pd.concat([pd.DataFrame(json_normalize(x)) for x in df['vehicle']],ignore_index=True)
vehicle_df.head()
### Combine the data
dff=df[['rating','renterTripsTaken','reviewCount','responseRate','newListing','freeDeliveryPromotion','instantBookDisplayed','deliveryLabel']]
rate_dff=rate_df['averageDailyPrice']
distance_dff=distanceWithUnit_df['scalar']
location_dff=location_df[['city','longitude','latitude','state']]
vehicle_dff=vehicle_df[['model','make','id','listingCreatedTime','year','type','automaticTransmission']]

pdList = [dff, rate_dff,distance_dff,location_dff,vehicle_dff]  # List of your dataframes
new_df = pd.concat(pdList,axis=1)
new_df.head()

new_df['listingCreatedTime'] = pd.to_datetime(new_df['listingCreatedTime'],unit='ms')
new_df.id.drop_duplicates()
len(new_df.id.drop_duplicates()) 
new_df.to_csv('car rental turo.csv')

"""## combine turo data with population data and brand level"""

pop=pd.read_csv('/content/drive/Shareddrives/ML&DD/final project/population data.csv')
pop.head()
new_df= pd.read_csv('/content/drive/Shareddrives/ML&DD/final project/car rental turo.csv')
level_df=pd.read_csv('/content/drive/Shareddrives/ML&DD/final project/Brand Level.csv')

new_df['state'].unique()

df0=new_df.copy()
df0.replace({'state' : {             'WA' : 'Washington', 'NM' : 'New Mexico', 'GA' : 'Georgia',
                                    'FL' :  'Florida' , 'TX' : 'Texas' , 'NC' : 'North Carolina',
                                    'SC' : 'South Carolina', 'CT' : 'Connecticut', 'MA' : 'Massachusetts',
                                    'ME' : 'Maine', 'AL' : 'Alabama', 'MT' : 'Montana',
                                    'TN' : 'Tennessee', 'KY' : 'Kentucky', 'ID' : 'Idaho' ,
                                    'UT' : 'Utah', 'MD' : 'Maryland', 'DC' : 'Washington' ,
                                    'IA' : 'Iowa', 'OH' : 'Ohio', 'CO' : 'Colorado' ,
                                    'VA' : 'Virginia', 'MI' : 'Michigan', 'NJ' : 'New Jersey' ,
                                    'IN' : 'Indiana', 'WI' : 'Wisconsin', 'KS' : 'Kansas' ,
                                    'MO' : 'Missouri', 'NV' : 'Nevada', 'CA' : 'California' ,
                                    'LA' : 'Louisiana', 'AR' : 'Arkansas', 'IL' : 'Illinois' ,
                                    'MS' : 'Mississippi', 'NH' : 'New Hampshire', 'MN' : 'Minnesota' ,
                                    'OK' : 'Oklahoma', 'NE' : 'Nebraska', 'OR' : 'Oregon' ,
                                    'PA' : 'Pennsylvania', 'DE' : 'Delaware', 'AZ' : 'Arizona' ,
                                    'WV' : 'West Virginia', 'RI' : 'Rhode Island', 'AK' : 'Alaska',
                                    'HI' : 'Hawaii', 'VT' : 'Vermont','ND' : 'North Dakota', 'WY' : 'Wyoming',
                                    'SD' : 'South Dakota'}},
              inplace=True)
df0 = pd.merge(df0, pop, how="left", on="state")
df0['listingyear']=pd.DatetimeIndex(df0['listingCreatedTime']).year

import numpy as np
lst = []
def createList(n):

    for i in range(n,n+20):
        lst.append(i)
    return lst
yearlist=createList(2000)
df0['population'] =0
for i in range(len(yearlist)):
    df0['population'] = (np.where(df0['listingyear'] == yearlist[i], df0[str(yearlist[i])] ,df0['population']  ))
    
df0=df0.drop(['2000', '2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019'], axis=1)

### Combine turo data with brand level information 
df0.merge(level_df, how='left', on='make')

# Import libraries
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()  # for plot styling
import numpy as np
import pandas as pd

# Connect to your google drive to access the data
from google.colab import drive
drive.mount('/content/drive')

# Import data into pandas dataframe
Turo = pd.read_csv('/content/drive/Shareddrives/ML&DD/final project/car rental turo data.csv')
level =  pd.read_csv('/content/drive/Shareddrives/ML&DD/final project/Brand Level.csv')

"""# Data Cleaning

### Take a brief look at the data
"""

Turo.head()

Turo.info()

"""### Extract year, month and date from timestamp column"""

# Create the column listingyear: year of the listing created
Turo['listingyear'] = pd.DatetimeIndex(Turo['listingCreatedTime']).year
Turo['listingyear'].head()

Turo['listingyear'].value_counts()

# Create the column listingmonth: month of the listing created
Turo['listingmonth'] = pd.DatetimeIndex(Turo['listingCreatedTime']).month
Turo['listingmonth'].head()

# Create the column listingCreatedDate: date of the listing created
Turo["listingCreatedDate"] = pd.to_datetime(Turo["listingCreatedTime"]).dt.date
Turo['listingCreatedDate'].head()

"""### Impute Missing Values"""

# Check whether there is missing values
print(Turo.isnull().sum())
# There are missing values in rating, responseRate, and deliveryLabel

plt.hist(Turo['rating'],bins=30)
plt.show() 
# Most listings get ratings around 4.5 - 5.0. We can assume the missing ratings
# are similar to the mean rating

# impute missing values in rating with mean
Turo['rating'].fillna((Turo['rating'].mean()), inplace=True)

# There is 0 in reponse rate. We can assume the missing value is 0. 
plt.hist(Turo['responseRate'],bins=30)
plt.show()

# Impute missing response rate with 0
Turo['responseRate'] = Turo['responseRate'].fillna(0)

Turo['deliveryLabel'].value_counts()
# This column has either "FREE DELIVERY" or "None". We can drop this 
# column because it is not meaningful

"""# EDA

### renterTripsTaken (representing the demand)
"""

Turo['renterTripsTaken'].describe()
# Min is 0. Max is 450, which may be an outlier.

plt.hist(Turo['renterTripsTaken'],bins=100)
plt.xlim(0,70)
plt.show()

"""## averageDailyPrice"""

Turo['averageDailyPrice'].describe()

plt.hist(Turo['averageDailyPrice'],bins=100)
plt.show() 
# left skewed, with an outlier = 1999

Turo[Turo["listingyear"] == 2017]

"""## Demand v.s. Price"""

plt.scatter(Turo['averageDailyPrice'], Turo['renterTripsTaken'])
plt.xlabel("AVG Daily Price")
plt.ylabel("Number of Trips")
plt.show() 
# There seems to be a curve pattern in the plot. 
# The higher the price,the lower the number of trips. 
# Therefore, using renterTripsTaken to represent demand makes sense.

"""# Demand over time

#### Overall trend
"""

demand_over_time = Turo[["renterTripsTaken","listingCreatedDate"]].groupby(by=["listingCreatedDate"]).sum()

plt.plot(demand_over_time.index, demand_over_time["renterTripsTaken"], color='tab:orange', label='Windspeed')
plt.xlabel("Date")
plt.ylabel("Number of Trips")
plt.show()
# 2012 has an extremely high trip demand. The demand increases until mid-2017 and then goes down.

"""### Seasonal trend by year"""

# Demand in 2018
demand_18 = Turo[Turo["listingyear"] == 2018]
demand_18 = demand_18[["renterTripsTaken","listingmonth"]].groupby(by=["listingmonth"]).sum()
plt.plot(demand_18.index, demand_18["renterTripsTaken"], color='tab:orange', label='Windspeed')
plt.xlabel("Month")
plt.ylabel("Number of Trips")
plt.show()

# Demand in 2017
demand_17 = Turo[Turo["listingyear"] == 2017]
demand_17 = demand_17[["renterTripsTaken","listingmonth"]].groupby(by=["listingmonth"]).sum()
plt.plot(demand_17 .index, demand_17["renterTripsTaken"], color='tab:orange', label='Windspeed')
plt.xlabel("Month")
plt.xticks(rotation = 90)
plt.ylabel("Number of Trips")
plt.show()

# Demand in 2016
demand_16 = Turo[Turo["listingyear"] == 2016]
demand_16 = demand_16[["renterTripsTaken","listingmonth"]].groupby(by=["listingmonth"]).sum()
plt.plot(demand_16 .index, demand_16["renterTripsTaken"], color='tab:orange', label='Windspeed')
plt.xlabel("Month")
plt.xticks(rotation = 90)
plt.ylabel("Number of Trips")
plt.show()

# Demand in 2015
demand_15 = Turo[Turo["listingyear"] == 2015]
demand_15 = demand_15[["renterTripsTaken","listingmonth"]].groupby(by=["listingmonth"]).sum()
plt.plot(demand_15 .index, demand_15["renterTripsTaken"], color='tab:orange', label='Windspeed')
plt.xlabel("Month")
plt.xticks(rotation = 90)
plt.ylabel("Number of Trips")
plt.show()

# Demand in 2014
demand_14 = Turo[Turo["listingyear"] == 2014]
demand_14 = demand_14[["renterTripsTaken","listingmonth"]].groupby(by=["listingmonth"]).sum()
plt.plot(demand_14 .index, demand_14["renterTripsTaken"], color='tab:orange', label='Windspeed')
plt.xlabel("Month")
plt.xticks(rotation = 90)
plt.ylabel("Number of Trips")
plt.show()

# Demand in 2013
demand_13 = Turo[Turo["listingyear"] == 2013]
demand_13 = demand_13[["renterTripsTaken","listingmonth"]].groupby(by=["listingmonth"]).sum()
plt.plot(demand_13 .index, demand_13["renterTripsTaken"], color='tab:orange', label='Windspeed')
plt.xlabel("Month")
plt.xticks(rotation = 90)
plt.ylabel("Number of Trips")
plt.show()

"""Every year, the seasonal change in demand is different."""

# Demand in 2012
demand_12 = Turo[Turo["listingyear"] == 2012]
demand_12 = demand_12[["renterTripsTaken","listingCreatedTime"]].groupby(by=["listingCreatedTime"]).sum()
demand_12.head()
# There are only two records in 2012. We may need to exclude 2012 from our analysis.

"""### Overall seasonal trend"""

demand_over_month = Turo[["renterTripsTaken","listingmonth"]].groupby(by=["listingmonth"]).sum()
plt.plot(demand_over_month.index, demand_over_month["renterTripsTaken"], color='tab:orange', label='Windspeed')
plt.xlabel("Month")
plt.ylabel("Number of Trips")
plt.show()

"""# Model building

### Use k-means clustering to create different car categories
"""

# Every year the demand looks different and 2017 has fairly large sample size, 
# so we focus on the 2017 data.
# Only keep the variables that are related to car itself: 
# Drop the geographical information like city, state, longitude and latitude. 
# Drop deliveryLabel because this column has only one meaningful value. 
# Drop the time variable such as listingyear, month and listingCreatedTime
# to strip the seasonality information 
# Drop renterTripsTaken, responserate, newListing, freeDeliveryPromotion, 
# instantBookDisplayed, year, population and model(600+ unique values, meaningless)

df_2017 = Turo[Turo["listingyear"] == 2017]
df_2017 = df_2017.drop(['renterTripsTaken','responseRate','newListing','freeDeliveryPromotion',
                        'instantBookDisplayed','city','state','deliveryLabel','id', 'listingCreatedTime',
                       'listingmonth', 'longitude', 'latitude',
                      'listingCreatedDate','model','population'], axis=1)
df_2017

# Create new feature age_before_listing representing how long the car has been 
# used before listed
df_2017['age_before_listing']=df_2017['listingyear']-df_2017['year']
df_2017 = df_2017.drop(['listingyear'], axis = 1)
df_2017 = df_2017.drop(['year'], axis = 1)

# create new feature brand level
df_2017=df_2017.merge(level, how='left', on='make')
df_2017.head()
df_2017

print(df_2017['type'].astype('category').value_counts())
# combine van and minivan since van only have 33 records
df_2017['type'] = df_2017['type'].replace(['van'],'minivan')
print(df_2017['type'].astype('category').value_counts())

X = df_2017
# Encoding binomial data
X['automaticTransmission'] = (np.where(X['automaticTransmission'] == 'True', 1,0 ))

# Encoding categorical variable: type
cleanup_type = {"type":   {"car": 1, "minivan": 2, 'truck': 4, 'suv':5}}
X = X.replace(cleanup_type)

# Encoding categorical variable: level
cleanup_level = {"Level":   {"Economy": 1, "Good": 2, 'Luxury': 3}}
X = X.replace(cleanup_level)
X = X.drop('make', axis = 1)
X

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

# Scale the data before fitting k-means
# In general, attribute scaling is important to be applied with K-means. 
# Most of the time, the standard Euclidean distance is used (as a distance 
# function of K-means) with the assumption that the attributes are normalized.
scaler = StandardScaler()
scaled_features = scaler.fit_transform(X)

"""## Choose K with the elbow method"""

# calculate SSE for a range of number of cluster
SSE_list = []
for i in range(1, 11):
    km = KMeans(
        n_clusters=i, init='random',
        n_init=10, max_iter=300,
        tol=1e-04, random_state=0
    )
    km.fit(X)
    SSE_list.append(km.inertia_)

# Elbow method to choose the optimal k
plt.plot(range(1, 11), SSE_list, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('SSE')
plt.show()

pip install kneed

# Using the b   elbow method, we chose 3 as the number of clusters
from kneed import KneeLocator
kl = KneeLocator(
   range(1, 11), SSE_list, curve="convex", direction="decreasing"
   )
print(kl.elbow)

"""### Create three clusters with K-means"""

# Fit k means with the number of clusters = 3
kmeans = KMeans(n_clusters=3)

# Label the data with 0, 1 and 2
y_kmeans = kmeans.fit_predict(scaled_features)

result = pd.DataFrame(y_kmeans)
result = result.rename(columns={0: 'Predict'})
X_pred = pd.concat([X,result],axis=1)
X_pred

X_pred['Predict'].value_counts()

centers = pd.DataFrame(kmeans.cluster_centers_)
names= X.columns
centers = scaler.inverse_transform(centers)
centers = pd.DataFrame(centers)
centers.columns = names
centers

X[['age_before_listing','Level']].groupby('Level').mean()

"""## Build Regression Model to estimate the demand-price elascticity and predict the optimal price"""

import matplotlib.pyplot as plt
import seaborn as sns; sns.set()  # for plot styling
import numpy as np

"""### Y = number of trips X = average daliy prie + every control variables + the number of available cars in the neighbourhood + listingmonth(seasonality)"""

### Prepare data for regression 
Regress_X=Turo.drop(['city','state','deliveryLabel','id'], axis=1)
Regress_X['Listing Difference']=Regress_X['listingyear']-Regress_X['year']

Regress_X.head()
Regress_X['newListing'] = (np.where(Regress_X['newListing'] == True, 1,0 ))
Regress_X['freeDeliveryPromotion'] = (np.where(Regress_X['freeDeliveryPromotion'] == True, 1,0 ))
Regress_X['instantBookDisplayed'] = (np.where(Regress_X['instantBookDisplayed'] == True, 1,0 ))
Regress_X['automaticTransmission'] = (np.where(Regress_X['automaticTransmission'] == True, 1,0 ))
Regress_X['responseRate'] = Regress_X['responseRate'].fillna(0)
Regress_X['rating'].fillna((Regress_X['rating'].mean()), inplace=True)
type_new = Regress_X['type'].astype('category')
type_new.head()
type_new=pd.DataFrame(type_new.cat.codes)
type_new=type_new.rename(columns={0: 'type_new'})
Regress_X1 = pd.concat([Regress_X,type_new],axis=1)

### Choose year 2017
Regress_X2=Regress_X1.loc[Regress_X1['listingyear']==2017]
Regress_X2=Regress_X2.reset_index(drop=True)

###listingmonth(seasonality)
Regress_X2=Regress_X2.drop(['make','model','type','listingyear','year',], axis=1)
Regress_X2['listingmonth'] = pd.DatetimeIndex(Regress_X2['listingCreatedTime']).month
Regress_X2=Regress_X2.drop(['listingCreatedTime'], axis=1)

### the number of available cars in the neighbourhood 
### We take the range of [-10,10] around the longitude latitude as the neighbourhood (nearby 10 miles cars)
Regress_X2['long_range_low']=Regress_X2['longitude']-0.14
Regress_X2['long_range_up']=Regress_X2['longitude']+0.14
Regress_X2['lat_range_low']=Regress_X2['latitude']-0.14
Regress_X2['lat_range_up']=Regress_X2['latitude']+0.14

Regress_X2

b=list()
for j in range(len(Regress_X2)):
    a=0
    for i in range(len(Regress_X2)):
        if (Regress_X2['longitude'][i] >= Regress_X2['long_range_low'][j]) == True and (Regress_X2['longitude'][i] <=Regress_X2['long_range_up'][j]) ==True and (Regress_X2['latitude'][i] <=Regress_X2['lat_range_low'][1]) ==True and (Regress_X2['latitude'][i] <=Regress_X2['lat_range_up'][1]) ==True:
            a=a+1
    print(j)
    b.append(a)

Regress_X2=Regress_X2.drop(['long_range_low','long_range_up','lat_range_low','lat_range_up','longitude','latitude'], axis=1)

Regress_X2 = pd.concat([Regress_X2,pd.DataFrame(b)],axis=1)
Regress_X2=Regress_X2.rename(columns={0: "num_neigh"})

Regress_X3 = pd.read_csv('/content/drive/Shareddrives/ML&DD/final project/Regress_X3.csv')

Regress_X3 = pd.concat([Regress_X3,X_pred['Predict']],axis=1)
Regress_X3

## Class 1
class1= Regress_X3[Regress_X3['Predict']==0]
class1.head()

import matplotlib.pyplot as plt
import statsmodels.api as sm

import statsmodels.formula.api as smf

plt.scatter(class1['averageDailyPrice'], class1['renterTripsTaken'])
plt.xlabel("AaverageDailyPrice")
plt.ylabel("Number of Trips")

### Class 1
class1=Regress_X3[Regress_X3['Predict']==0]
class1.head()
X_1=class1.drop(['Predict'], axis=1)
X_1=X_1.rename(columns={'Listing Difference': 'ListingDifference'})
model2 = smf.ols('np.log(renterTripsTaken+1) ~ rating + reviewCount + responseRate + ListingDifference + population + listingmonth + num_neigh + np.log(averageDailyPrice)', data=X_1).fit()
print(model2.summary())

model2.params

import matplotlib.pyplot as plt
import statsmodels.api as sm

import statsmodels.formula.api as smf
class1['renterTripsTaken']= np.exp(model2.params[0] + model2.params[-1] * np.log(class1['averageDailyPrice']))
plt.scatter(class1['averageDailyPrice'], class1['renterTripsTaken'])
plt.xlabel("log_AaverageDailyPrice")
plt.ylabel("log_Number of Trips")

### Class 2
class2=Regress_X3[Regress_X3['Predict']==1]

X_2=class2.drop(['Predict'], axis=1)
X_2=X_2.rename(columns={'Listing Difference': 'ListingDifference'})
model3 = smf.ols('np.log(renterTripsTaken+1) ~ rating + reviewCount + responseRate + ListingDifference + population + listingmonth + num_neigh + np.log(averageDailyPrice)', data=X_2).fit()
print(model3.summary())

import matplotlib.pyplot as plt
import statsmodels.api as sm

import statsmodels.formula.api as smf
class2['renterTripsTaken']= np.exp(model3.params[0] + model3.params[-1] * np.log(class2['averageDailyPrice']))
plt.scatter(class2['averageDailyPrice'], class2['renterTripsTaken'])
plt.xlabel("AaverageDailyPrice")
plt.ylabel("Number of Trips")

### Class 3
class3=Regress_X3[Regress_X3['Predict']==2]

X_3=class3.drop(['Predict'], axis=1)
X_3=X_3.rename(columns={'Listing Difference': 'ListingDifference'})
model4 = smf.ols('np.log(renterTripsTaken+1) ~ rating + reviewCount + responseRate + ListingDifference + population + listingmonth + num_neigh + np.log(averageDailyPrice)', data=X_3).fit()
print(model4.summary())

import matplotlib.pyplot as plt
import statsmodels.api as sm

import statsmodels.formula.api as smf
class3['renterTripsTaken']= np.exp(model4.params[0] + model4.params[-1] * np.log(class3['averageDailyPrice']))
plt.scatter(class3['averageDailyPrice'], class3['renterTripsTaken'])
plt.xlabel("AaverageDailyPrice")
plt.ylabel("Number of Trips")

class1['averageDailyPrice'].mean()

